{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257d5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    GridSearchCV,\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import (\n",
    "    VotingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.stats import pearsonr, uniform, randint\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c75dd8",
   "metadata": {},
   "source": [
    "## Quick Summary:\n",
    "\n",
    "- [1. Loading data & Cleaning](#1-loading-data--cleaning)\n",
    "    - [1.1 Loading ready-for-model dataset](#1-loading-data--cleaning)\n",
    "    - [1.2 Cleaning, train-test splitting](#12-cleaning-train-test-splitting)\n",
    "\n",
    "- [2. Classification Model Training ](#2-classification-model-training)\n",
    "    - [2.1 Training all models](#21-training-all-models)\n",
    "        - [2.1.1 Baseline - Logistic Regression](#211-baseline---logistic-regression)\n",
    "        - [2.1.2 SVM Classifier](#212-svm-classifier)\n",
    "        - [2.1.3 Decision Tree](#213-decision-tree) \n",
    "        - [2.1.4 Random Forest](#214-random-forest)\n",
    "        - [2.1.5 Gradient Boosting](#215-gradient-boosting) \n",
    "\n",
    "        \n",
    "    - [2.2 Comparison of Classifiers](#22-comparison-of-classifiers)\n",
    "        - [2.2.1 Comparison Table](#221-comparison-table)\n",
    "        - [2.2.2 Comparison ROC curve](#222-comparison-roc-curve)\n",
    "- [3. Model extraction](#3-model-extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354b694",
   "metadata": {},
   "source": [
    "Important things to keep in mind:\n",
    "- False Positive is more dangerous, because it will defect your software and crash. \n",
    "- we can add lightweight classifier later to see how likely is the maintainers username is spam_likely - improve the main ML model. Maintainers likely to create username in pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf9a95",
   "metadata": {},
   "source": [
    "## 1. Loading data & Cleaning\n",
    "#### 1.1 Loading ready-for-model dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3dc7208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15587 entries, 0 to 15586\n",
      "Data columns (total 52 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   pkg_name                            15587 non-null  object \n",
      " 1   n_name_len                          15587 non-null  int64  \n",
      " 2   has_digit_in_name                   15587 non-null  bool   \n",
      " 3   has_dash_or_underscore              15587 non-null  bool   \n",
      " 4   cat_name_case                       15587 non-null  object \n",
      " 5   n_summary_len                       15587 non-null  int64  \n",
      " 6   n_desc_len                          15587 non-null  int64  \n",
      " 7   n_desc_lines                        15587 non-null  int64  \n",
      " 8   has_code_block_in_desc              15587 non-null  bool   \n",
      " 9   n_urls_in_desc                      15587 non-null  int64  \n",
      " 10  has_suspicious_kw                   15587 non-null  bool   \n",
      " 11  pct_non_ascii_desc                  15587 non-null  float64\n",
      " 12  t_age_first_release_days            15587 non-null  int64  \n",
      " 13  t_age_last_release_days             15587 non-null  int64  \n",
      " 14  n_versions                          15587 non-null  int64  \n",
      " 15  t_median_release_gap_days           15587 non-null  int64  \n",
      " 16  has_single_release                  15587 non-null  bool   \n",
      " 17  cat_weekday_of_last_release         15587 non-null  object \n",
      " 18  n_maintainers                       15587 non-null  int64  \n",
      " 19  pct_free_email_domains              15587 non-null  float64\n",
      " 20  has_disposable_email                15587 non-null  float64\n",
      " 21  has_missing_author                  15587 non-null  float64\n",
      " 22  has_homepage                        15587 non-null  float64\n",
      " 23  has_repo_url                        15587 non-null  float64\n",
      " 24  cat_repo_host                       15587 non-null  object \n",
      " 25  has_issue_tracker                   15587 non-null  float64\n",
      " 26  has_docs_url                        15587 non-null  float64\n",
      " 27  has_license                         15587 non-null  float64\n",
      " 28  cat_license_family                  15587 non-null  object \n",
      " 29  n_classifiers                       15587 non-null  int64  \n",
      " 30  has_prog_lang_classifier            15587 non-null  float64\n",
      " 31  has_typing_classifier               15587 non-null  float64\n",
      " 32  n_distributions                     15587 non-null  int64  \n",
      " 33  n_requires                          15587 non-null  int64  \n",
      " 34  has_extras                          15587 non-null  float64\n",
      " 35  n_downloads_7d                      15587 non-null  int64  \n",
      " 36  n_downloads_30d                     15587 non-null  int64  \n",
      " 37  n_dependents_est                    15587 non-null  int64  \n",
      " 38  rule_no_repo_low_desc_len           15587 non-null  int64  \n",
      " 39  rule_suspicious_name_and_dep        15587 non-null  int64  \n",
      " 40  is_spam                             15587 non-null  int64  \n",
      " 41  n_lev_dist_to_top1                  15587 non-null  int64  \n",
      " 42  n_lev_dist_to_alias                 15587 non-null  int64  \n",
      " 43  sim_tfidf_to_legit_centroid         15587 non-null  float64\n",
      " 44  dist_embed_to_legit_desc            15587 non-null  float64\n",
      " 45  n_pkgs_by_maintainers_30d           15587 non-null  int64  \n",
      " 46  n_low_download_pkgs_by_maintainers  15587 non-null  int64  \n",
      " 47  n_latest_project_urls               15587 non-null  int64  \n",
      " 48  has_dependency_to_top_brand         15587 non-null  int64  \n",
      " 49  min_dep_lev_to_brand                15587 non-null  int64  \n",
      " 50  has_dependency_lev_close_to_brand   15587 non-null  int64  \n",
      " 51  t_time_of_day                       15587 non-null  object \n",
      "dtypes: bool(5), float64(14), int64(27), object(6)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "from feature_engineering.load_data import load_json\n",
    "\n",
    "df, _ = load_json(\"../data/20251207-bq-results-with-mid-pkgs.jsonl\", lines=True)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c053c",
   "metadata": {},
   "source": [
    "#### 1.2 Cleaning, train-test splitting\n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y variables\n",
    "drop_cols = [\"pkg_name\", \"is_spam\"]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "y = df[\"is_spam\"].astype(int)\n",
    "\n",
    "# seperating categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# for col in categorical_cols:\n",
    "#     print(col, X[col].unique())\n",
    "\n",
    "# dummy encoding the dataset\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"),\n",
    "            categorical_cols,\n",
    "        )\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# train-test split\n",
    "# ----------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# encoding cat values:\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "# Get the new feature names (encoded categories + passed-through numerical columns)\n",
    "feature_names_out = preprocessor.get_feature_names_out().tolist()\n",
    "\n",
    "# Convert the NumPy arrays back to DataFrames\n",
    "X_train_encoded_df = pd.DataFrame(\n",
    "    X_train_encoded, columns=feature_names_out, index=X_train.index\n",
    ")\n",
    "X_test_encoded_df = pd.DataFrame(\n",
    "    X_test_encoded, columns=feature_names_out, index=X_test.index\n",
    ")\n",
    "scaler = StandardScaler().fit(X_train_encoded_df)\n",
    "X_train_encoded_df = scaler.transform(X_train_encoded_df)\n",
    "X_test_encoded_df = scaler.transform(X_test_encoded_df)\n",
    "\n",
    "X_train_const = add_constant(X_train_encoded_df, has_constant=\"add\")\n",
    "X_test_const = add_constant(X_test_encoded_df, has_constant=\"add\")\n",
    "print(X_train_const.shape, X_test_const.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1ca33",
   "metadata": {},
   "source": [
    "# 2. Classification Model Training\n",
    "### 2.1 Training all models\n",
    "\n",
    "##### 2.1.1 Baseline - Logistic Regression\n",
    "\n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0547e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2.1.1. Logistic Regression\n",
    "# --------------------------------------------------\n",
    "# fit the regression:\n",
    "print(\"\\n--- Model Evaluation (Logistic Regression) ---\")\n",
    "log_reg_base = LogisticRegression()\n",
    "log_reg_base.fit(X_train_const, y_train)\n",
    "y_train_pred_log = log_reg_base.predict(X_train_const)\n",
    "y_test_pred_log = log_reg_base.predict(X_test_const)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy_log = accuracy_score(y_train, y_train_pred_log)\n",
    "test_accuracy_log = accuracy_score(y_test, y_test_pred_log)\n",
    "print(f\"Training Accuracy: {train_accuracy_log:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_log:.4f}\")\n",
    "\n",
    "print(\"\\n--- Classification Report (Test Set) ---\")\n",
    "print(classification_report(y_test, y_test_pred_log))\n",
    "# ROC and AUC:\n",
    "fpr_log, tpr_log, thresh_log = roc_curve(y_test, y_test_pred_log)\n",
    "auc_log = roc_auc_score(y_test, y_test_pred_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc40fa",
   "metadata": {},
   "source": [
    "##### 2.1.2 SVM Classifier\n",
    "\n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2.1.2. Initialize the SVM Model - linear kernel\n",
    "# --------------------------------------------------\n",
    "# 1. Initialize the SVM model\n",
    "svm_model = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
    "\n",
    "# 2. Fit the SVM model\n",
    "# used the encoded data WITHOUT the added constant, as SVC handles the intercept.\n",
    "svm_model.fit(X_train_encoded_df, y_train)\n",
    "\n",
    "# 3. Predict on Training and Testing Sets\n",
    "y_train_pred_svm = svm_model.predict(X_train_encoded_df)\n",
    "y_test_pred_svm = svm_model.predict(X_test_encoded_df)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "print(\"\\n--- Model Evaluation (Support Vector Machine) ---\")\n",
    "\n",
    "train_accuracy_svm = accuracy_score(y_train, y_train_pred_svm)\n",
    "test_accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "print(f\"Training Accuracy: {train_accuracy_svm:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_svm:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred_svm))\n",
    "# ROC and AUC:\n",
    "fpr_svm, tpr_svm, thresh_svm = roc_curve(y_test, y_test_pred_svm)\n",
    "auc_svm = roc_auc_score(y_test, y_test_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c50545",
   "metadata": {},
   "source": [
    "##### 2.1.3 Decision Tree\n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2.1.3. CART\n",
    "# --------------------------------------------------\n",
    "grid_values = {\n",
    "    \"ccp_alpha\": np.linspace(0, 0.10, 201),\n",
    "    \"min_samples_leaf\": [5],\n",
    "    \"min_samples_split\": [20],\n",
    "    \"max_depth\": [30],\n",
    "    \"random_state\": [88],\n",
    "}\n",
    "\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc_cv_acc = GridSearchCV(\n",
    "    dtc, param_grid=grid_values, scoring=\"accuracy\", cv=10, verbose=1\n",
    ")  # default scoring metric to optimize is accuracy, used as default if none given.\n",
    "dtc_cv_acc.fit(X_train_const, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beea1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Retrieve the Best Model ---\n",
    "# The best_estimator_ attribute holds the final, trained model with the optimal parameters.\n",
    "dtc_best_model = dtc_cv_acc.best_estimator_\n",
    "\n",
    "# Print the best parameters found by GridSearchCV for verification\n",
    "print(\"Best parameters found:\")\n",
    "print(dtc_cv_acc.best_params_)\n",
    "print(\n",
    "    \"Grid best parameter ccp_alpha (max. accuracy): \",\n",
    "    dtc_cv_acc.best_params_[\"ccp_alpha\"],\n",
    ")\n",
    "print(\"Grid best score (accuracy): \", dtc_cv_acc.best_score_)\n",
    "# --- 2. Make Predictions ---\n",
    "# Predict classes (0 or 1)\n",
    "y_train_pred_dtc = dtc_best_model.predict(X_train_const)\n",
    "y_test_pred_dtc = dtc_best_model.predict(X_test_const)\n",
    "\n",
    "# Predict probabilities for ROC/AUC (optional, but often better than raw classes for AUC)\n",
    "# We assume the Decision Tree classifier supports predict_proba.\n",
    "try:\n",
    "    y_test_proba_dtc = dtc_best_model.predict_proba(X_test_const)[:, 1]\n",
    "except AttributeError:\n",
    "    # Fallback if predict_proba is not available, though it should be for DTC\n",
    "    y_test_proba_dtc = y_test_pred_dtc\n",
    "\n",
    "\n",
    "# ---  Evaluation ---\n",
    "print(\"\\n--- Model Evaluation (Decision Tree Classifier) ---\")\n",
    "# Compute Accuracy\n",
    "train_accuracy_dtc = accuracy_score(y_train, y_train_pred_dtc)\n",
    "test_accuracy_dtc = accuracy_score(y_test, y_test_pred_dtc)\n",
    "print(f\"Training Accuracy: {train_accuracy_dtc:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_dtc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred_dtc))\n",
    "# Compute ROC and AUC\n",
    "fpr_dtc, tpr_dtc, thresh_dtc = roc_curve(y_test, y_test_proba_dtc)\n",
    "auc_dtc = roc_auc_score(y_test, y_test_proba_dtc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e6a4ff",
   "metadata": {},
   "source": [
    "#### 2.1.4. Random Forest\n",
    "\n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 2.1.4. Random Forest\n",
    "# --------------------------------------------------\n",
    "grid_values = {\n",
    "    \"max_features\": np.linspace(1, 18, 18, dtype=\"int32\"),\n",
    "    \"min_samples_leaf\": [5],\n",
    "    \"n_estimators\": [500],\n",
    "    \"random_state\": [88],\n",
    "}\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "# Note: here we set verbose=2 to keep track of the progress (the running time) of the cross validation.\n",
    "cv = KFold(n_splits=5, random_state=333, shuffle=True)\n",
    "rf_cv = GridSearchCV(rf, param_grid=grid_values, scoring=\"accuracy\", cv=cv, verbose=2)\n",
    "rf_cv.fit(X_train_const, y_train)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Best params:\", rf_cv.best_params_)\n",
    "print(\"Best CV accuracy:\", rf_cv.best_score_)\n",
    "print(\"Time:\", round(toc - tic, 2), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69241c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest with chosen hyperparameters\n",
    "max_features = rf_cv.cv_results_[\"param_max_features\"].data\n",
    "cv_scores = rf_cv.cv_results_[\"mean_test_score\"]\n",
    "best_idx = np.argmax(cv_scores)\n",
    "\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=rf_cv.best_params_.get(\"n_estimators\", 100),\n",
    "    max_features=max_features[best_idx],\n",
    "    min_samples_leaf=rf_cv.best_params_.get(\"min_samples_leaf\", 1),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_final.fit(X_train_const, y_train)\n",
    "y_train_pred_rf = rf_final.predict(X_train_const)\n",
    "y_test_pred_rf = rf_final.predict(X_test_const)\n",
    "# Evaluate the model\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_log)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_log)\n",
    "print(f\"Training Accuracy: {train_accuracy_rf:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_rf:.4f}\")\n",
    "\n",
    "print(\"\\n--- Classification Report (Test Set) ---\")\n",
    "print(classification_report(y_test, y_test_pred_log))\n",
    "# ROC and AUC:\n",
    "fpr_rf, tpr_rf, thresh_rf = roc_curve(y_test, y_test_pred_rf)\n",
    "auc_rf = roc_auc_score(y_test, y_test_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ea233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of feature names used for training\n",
    "# We assume the 'const' term is the first column as added by statsmodels.api.add_constant\n",
    "feature_names = [\"const\"] + preprocessor.get_feature_names_out().tolist()\n",
    "\n",
    "# 2. Extract importances from the fitted model\n",
    "importances = rf_final.feature_importances_\n",
    "\n",
    "# 3. Create the DataFrame\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": feature_names, \"Importance\": importances}\n",
    ")\n",
    "\n",
    "# 4. Sort and Display Results\n",
    "# Drop the 'const' feature as its importance is irrelevant/misleading\n",
    "feature_importance_df = feature_importance_df[\n",
    "    feature_importance_df[\"Feature\"] != \"const\"\n",
    "]\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n--- Top 15 Feature Importances (Random Forest) ---\")\n",
    "print(feature_importance_df.head(15).to_markdown(index=False))\n",
    "\n",
    "# 5. Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_df.head(15)\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=top_features, color=\"#f39c12\")\n",
    "plt.title(\"Top 15 Feature Importances for PyPI Spam Detection\")\n",
    "plt.xlabel(\"Feature Importance (Gini Importance)\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2324d5e",
   "metadata": {},
   "source": [
    "##### 2.1.5 Gradient Boosting\n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fc86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Random search space\n",
    "# param_dist = {\n",
    "#     \"n_estimators\": randint(50, 400),\n",
    "#     \"learning_rate\": uniform(0.001, 0.5),\n",
    "#     \"max_depth\": randint(2, 7),\n",
    "#     \"subsample\": uniform(0.5, 0.5),     # searches from 0.5 to 1.0\n",
    "#     \"min_samples_split\": randint(2, 20),\n",
    "#     \"min_samples_leaf\": randint(1, 20)\n",
    "# }\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 300),  # fewer trees, usually 100â€“200 is enough\n",
    "    \"learning_rate\": uniform(0.01, 0.2),  # narrower, realistic range\n",
    "    \"max_depth\": randint(2, 5),  # shallow trees work best for GB\n",
    "}\n",
    "\n",
    "\n",
    "gb_random = RandomizedSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # number of random trials. canmake it 30 for random search later.\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "gb_random.fit(X_train_const, y_train)\n",
    "\n",
    "# --------------------------------------\n",
    "# Best params and model\n",
    "# --------------------------------------\n",
    "print(\"\\nBest Params:\")\n",
    "print(gb_random.best_params_)\n",
    "\n",
    "print(\"\\nBest CV Score:\", gb_random.best_score_)\n",
    "\n",
    "# Best model object:\n",
    "gb_best_model = gb_random.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7651a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Extract best hyperparameters from CV\n",
    "# -------------------------------------------\n",
    "best_params = gb_best_model.get_params()\n",
    "learning_rate = best_params.get(\"learning_rate\", 0.1)\n",
    "n_estimators = best_params.get(\"n_estimators\", 100)\n",
    "max_depth = best_params.get(\"max_depth\", 3)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Fit final Gradient Boosting model\n",
    "# -------------------------------------------\n",
    "gb_final = GradientBoostingClassifier(\n",
    "    learning_rate=learning_rate,\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "gb_final.fit(X_train_const, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_gb = gb_final.predict(X_train_const)\n",
    "y_test_pred_gb = gb_final.predict(X_test_const)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Evaluate model\n",
    "# -------------------------------------------\n",
    "train_accuracy_gb = accuracy_score(y_train, y_train_pred_gb)\n",
    "test_accuracy_gb = accuracy_score(y_test, y_test_pred_gb)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy_gb:.4f}\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy_gb:.4f}\")\n",
    "\n",
    "print(\"\\n--- Classification Report (Test Set) ---\")\n",
    "print(classification_report(y_test, y_test_pred_gb))\n",
    "\n",
    "# -------------------------------------------\n",
    "# ROC & AUC\n",
    "# -------------------------------------------\n",
    "y_test_proba_gb = gb_final.predict_proba(X_test_const)[:, 1]\n",
    "\n",
    "fpr_gb, tpr_gb, thresh_gb = roc_curve(y_test, y_test_proba_gb)\n",
    "auc_gb = roc_auc_score(y_test, y_test_proba_gb)\n",
    "\n",
    "print(f\"AUC-ROC (GB): {auc_gb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99af5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b252660",
   "metadata": {},
   "source": [
    "### 2.2 Comparison of Classifiers\n",
    "#### 2.2.1 Comparison Table \n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac958361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def structure_model_metrics(\n",
    "    y_train_true, y_test_true, y_train_pred, y_test_pred, train_acc, test_acc, auc_score\n",
    "):\n",
    "    \"\"\"Calculates and structures key classification metrics for the positive class (1).\"\"\"\n",
    "\n",
    "    # Calculate Recall for the positive class (Class 1) for both train and test\n",
    "    recall_train = recall_score(\n",
    "        y_train_true, y_train_pred, pos_label=1, zero_division=0\n",
    "    )\n",
    "    recall_test = recall_score(y_test_true, y_test_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"Train Accuracy\": train_acc,\n",
    "        \"Test Accuracy\": test_acc,\n",
    "        \"Train Recall (Class 1)\": recall_train,\n",
    "        \"Test Recall (Class 1)\": recall_test,\n",
    "        \"Test AUC\": auc_score,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 2. Calculate Metrics for Each Model ---\n",
    "\n",
    "# A. Logistic Regression Metrics\n",
    "logreg_metrics = structure_model_metrics(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_train_pred_log,\n",
    "    y_test_pred_log,\n",
    "    train_accuracy_log,\n",
    "    test_accuracy_log,\n",
    "    auc_log,\n",
    ")\n",
    "\n",
    "# B. SVM Classifier Metrics\n",
    "svm_metrics = structure_model_metrics(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_train_pred_svm,\n",
    "    y_test_pred_svm,\n",
    "    train_accuracy_svm,\n",
    "    test_accuracy_svm,\n",
    "    auc_svm,\n",
    ")\n",
    "\n",
    "# C. Decision Tree Metrics\n",
    "# NOTE: The AUC for DTC uses y_test_proba_dtc (probabilities), which is the standard,\n",
    "# while the other predictions use classes, which is fine for direct comparison.\n",
    "dtc_metrics = structure_model_metrics(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_train_pred_dtc,\n",
    "    y_test_pred_dtc,\n",
    "    train_accuracy_dtc,\n",
    "    test_accuracy_dtc,\n",
    "    auc_dtc,\n",
    ")\n",
    "\n",
    "# D. Random Forest\n",
    "dtc_metrics = structure_model_metrics(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_train_pred_rf,\n",
    "    y_test_pred_rf,\n",
    "    train_accuracy_rf,\n",
    "    test_accuracy_rf,\n",
    "    auc_rf,\n",
    ")\n",
    "\n",
    "# D. Random Forest\n",
    "rf_metrics = structure_model_metrics(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_train_pred_rf,\n",
    "    y_test_pred_rf,\n",
    "    train_accuracy_rf,\n",
    "    test_accuracy_rf,\n",
    "    auc_rf,\n",
    ")\n",
    "\n",
    "# D.Gradient Boosting\n",
    "# gradient_metrics = structure_model_metrics(\n",
    "#     y_train, y_test, y_train_pred_gb, y_test_pred_gb,\n",
    "#     train_accuracy_gb, test_accuracy_gb, auc_gb\n",
    "# )\n",
    "\n",
    "\n",
    "placeholder_metrics = {\n",
    "    \"Train Accuracy\": np.nan,\n",
    "    \"Test Accuracy\": np.nan,\n",
    "    \"Train Recall (Class 1)\": np.nan,\n",
    "    \"Test Recall (Class 1)\": np.nan,\n",
    "    \"Test AUC\": np.nan,\n",
    "}\n",
    "\n",
    "# --- 3. Consolidate into the Final DataFrame ---\n",
    "all_results = {\n",
    "    \"Logistic Regression\": logreg_metrics,\n",
    "    \"SVM Classifier\": svm_metrics,\n",
    "    \"Decision Tree\": dtc_metrics,\n",
    "    \"Random Forest\": rf_metrics,\n",
    "    \"Gradient Boosting\": placeholder_metrics.copy(),\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "\n",
    "# --- 4. Display the Formatted Table ---\n",
    "print(\n",
    "    \"----------------------------------------------------\\n--- CONSOLIDATED CLASSIFICATION MODEL COMPARISON ---\"\n",
    ")\n",
    "\n",
    "# Format all columns to 3 decimal places\n",
    "results_df_formatted = results_df.applymap(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"-\")\n",
    "\n",
    "# Print the final comparison table\n",
    "# print(results_df_formatted.to_markdown(index=True))\n",
    "results_df_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321bffa",
   "metadata": {},
   "source": [
    "##### 2.2.2 Comparison ROC curve\n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28854289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Comparison Graph:\n",
    "# --- Setup for Multiple Plots ---\n",
    "fig, axes = plt.subplots(1, 5, figsize=(30, 6))\n",
    "\n",
    "# Ensure axes is flat for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# --- Plot 1: Logistic Regression ---\n",
    "ax = axes[0]\n",
    "ax.plot(fpr_log, tpr_log, label=f\"ROC curve (AUC = {auc_log:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"Random guess\")\n",
    "ax.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "ax.set_ylabel(\"True Positive Rate (TPR)\")\n",
    "ax.set_title(\"2. ROC Curve - Logistic Regression\")\n",
    "ax.grid(True)\n",
    "\n",
    "# --- Plot 1: SVM ROC Curve ---\n",
    "ax = axes[1]\n",
    "ax.plot(fpr_svm, tpr_svm, label=f\"ROC curve (AUC = {auc_svm:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"Random guess\")\n",
    "ax.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "ax.set_ylabel(\"True Positive Rate (TPR)\")\n",
    "ax.set_title(\"2. ROC Curve - SVM\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True)\n",
    "\n",
    "# --- Plot 3:  ---\n",
    "ax = axes[2]\n",
    "ax.set_title(\"3. ROC Curve - Decision Tree\")\n",
    "ax.plot(fpr_dtc, tpr_dtc, label=f\"ROC curve (AUC = {auc_dtc:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"Random guess\")\n",
    "ax.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "ax.set_ylabel(\"True Positive Rate (TPR)\")\n",
    "ax.grid(True)\n",
    "\n",
    "# --- Plot 4: ---\n",
    "ax = axes[3]\n",
    "ax.set_title(\"4. ROC Curve - Random Forest\")\n",
    "ax.plot(fpr_rf, tpr_rf, label=f\"ROC curve (AUC = {auc_rf:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"Random guess\")\n",
    "ax.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "ax.set_ylabel(\"True Positive Rate (TPR)\")\n",
    "ax.grid(True)\n",
    "\n",
    "# --- Plot 5: ---\n",
    "ax = axes[4]\n",
    "ax.set_title(\"5. ROC Curve - Gradient Boosting\")\n",
    "ax.plot(fpr_gb, tpr_gb, label=f\"ROC curve (AUC = {auc_gb:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"Random guess\")\n",
    "ax.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "ax.set_ylabel(\"True Positive Rate (TPR)\")\n",
    "ax.grid(True)\n",
    "\n",
    "# --- Final Display ---\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf89163",
   "metadata": {},
   "source": [
    "### 3. Model Extraction\n",
    "\n",
    "[Click here to go to 'Quick Summary'](#quick-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory and generate timestamp\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def save_model(model, base_name, models_dir, timestamp):\n",
    "    \"\"\"Save model with timestamp to models directory.\"\"\"\n",
    "    filename = f\"{base_name}_{timestamp}.joblib\"\n",
    "    filepath = models_dir / filename\n",
    "    joblib.dump(model, filepath)\n",
    "    # Also save as the 'latest' version without timestamp\n",
    "    latest_filepath = models_dir / f\"{base_name}.joblib\"\n",
    "    joblib.dump(model, latest_filepath)\n",
    "    print(f\"Saved: {filepath} (and {latest_filepath})\")\n",
    "    return filepath\n",
    "\n",
    "# Save preprocessor and scaler\n",
    "save_model(preprocessor, \"fitted_preprocessor\", models_dir, timestamp)\n",
    "save_model(scaler, \"fitted_scaler\", models_dir, timestamp)\n",
    "\n",
    "# Save all classifier models\n",
    "save_model(log_reg_base, \"log_reg_spam_classifier\", models_dir, timestamp)\n",
    "save_model(svm_model, \"svm_linear_spam_classifier\", models_dir, timestamp)\n",
    "save_model(dtc_best_model, \"dtc_spam_classifier\", models_dir, timestamp)\n",
    "save_model(rf_final, \"rf_spam_classifier\", models_dir, timestamp)\n",
    "save_model(gb_final, \"gb_spam_classifier\", models_dir, timestamp)\n",
    "\n",
    "print(f\"\\nAll models saved with timestamp: {timestamp}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
